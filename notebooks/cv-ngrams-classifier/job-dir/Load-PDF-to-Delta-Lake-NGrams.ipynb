{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Miner to JSON done !\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "###### Import Data : FROM PDF to Delta-Lake   ######\n",
    "####################################################\n",
    "#####\n",
    "####################################################\n",
    "\n",
    "# Get data From Folder\n",
    "#\n",
    "from datetime import datetime\n",
    "\n",
    "#datepath=\"2022-02-25\"\n",
    "datepath=datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "job_dir=\"/home/notebookuser/notebooks/cv-ngrams-classifier/job-dir/\"\n",
    "pdf_daily_path =job_dir+\"data/raw_pdf/dt=\"+datepath+\"/\" \n",
    "#\n",
    "json_daily_path=job_dir+\"data/raw_json/dt=\"+datepath+\"/\"\n",
    "delta_json_structure=job_dir+\"data/delta/json-cv-pdf\"\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#\n",
    "import io\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "#\n",
    "import json\n",
    "import os\n",
    "from lib.local_pdfminer.json_exporter import export_as_json\n",
    "from lib.local_pdfminer.json_exporter import export_as_json_page_n\n",
    "#\n",
    "from lib.local_sh.functions import copy_raw_sh_to_local\n",
    "from lib.local_sh.functions import copy_local_to_raw_sh\n",
    "#\n",
    "import os\n",
    "pdf_files=[val for sublist in [[os.path.join(i[0], j) for j in i[2]] for i in os.walk(pdf_daily_path)] for val in sublist]\n",
    "# Meta comment to ease selecting text\n",
    "#\n",
    "os.system('mkdir -p '+json_daily_path)\n",
    "for i, pdf_file in enumerate(pdf_files):    \n",
    "    json_path = json_daily_path+\"extract-\"+datepath+\"-\"+str(i)+\".json\"\n",
    "    data_csv=export_as_json_page_n(pdf_file, json_path)\n",
    "#\n",
    "### Input File in folder :  json_daily_path\n",
    "##\n",
    "####\n",
    "#\n",
    "print(\"PDF Miner to JSON done !\")\n",
    "#\n",
    "##############################Execution##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- pages: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- p_content: string (nullable = true)\n",
      " |    |    |-- page_n: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|            filename|               pages|\n",
      "+--------------------+--------------------+\n",
      "|SR992-21 Woodland...|[[\f",
      ", 1], [\f",
      ", 2], ...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- pages: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- p_content: string (nullable = true)\n",
      " |    |    |-- page_n: string (nullable = true)\n",
      "\n",
      "Data Load Done!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "### Input File in folder :  json_daily_path\n",
    "#\n",
    "job_dir=\"/home/notebookuser/notebooks/cv-ngrams-classifier/job-dir/\"\n",
    "#\n",
    "#datepath=\"2022-02-25\"\n",
    "datepath=datetime.today().strftime('%Y-%m-%d')\n",
    "#\n",
    "json_daily_path=job_dir+\"data/raw_json/dt=\"+datepath+\"/\"\n",
    "delta_json_structure=job_dir+\"data/delta/json-cv-pdf\"\n",
    "#\n",
    "##############################Execution##########################\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "# Start Debug!!!\n",
    "import os\n",
    "os.system(\"echo $JAVA_HOME >  spark-job.log\")\n",
    "#os.system(\"export PYSPARK_SUBMIT_ARGS='---master local[*] --packages io.delta:delta-core_2.11:0.5.0 pyspark-shell' \")\n",
    "os.system(\"echo $PYSPARK_SUBMIT_ARGS >>  spark-job.log\")\n",
    "#os.system(\"echo 'javac -version' | xargs -0 -l -d '\\\\n' >> spark-job.log \")\n",
    "# End Debug!!!\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"Business_Dictionary-Ngrams_CVs-Delta\")\n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "# Join with Internal Curation Data in urltopredict staged folder\n",
    "from pyspark.sql import functions as F\n",
    "### remove viewerID\n",
    "internaldata_df1=sqlContext.read.json(json_daily_path).persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#\n",
    "#\n",
    "internaldata_df1.printSchema()\n",
    "#\n",
    "internaldata_df1.show(8)\n",
    "#\n",
    "df1=internaldata_df1.filter(\"filename IS NOT NULL\").filter(\"pages IS NOT NULL\").persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#\n",
    "#df1.show(1,0)\n",
    "df1.printSchema()\n",
    "##\n",
    "df1.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(delta_json_structure)\n",
    "##\n",
    "####\n",
    "#\n",
    "print(\"Data Load Done!\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- pages: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- p_content: string (nullable = true)\n",
      " |    |    |-- page_n: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- pagei: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Filename: string (nullable = true)\n",
      " |-- pagei: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+----------------------------------+-----+\n",
      "|Filename                          |pagei|\n",
      "+----------------------------------+-----+\n",
      "|SR992-21 Woodlands Ave report v2.0|[]   |\n",
      "|SR992-21 Woodlands Ave report v2.0|[]   |\n",
      "|SR992-21 Woodlands Ave report v2.0|[]   |\n",
      "|SR992-21 Woodlands Ave report v2.0|[]   |\n",
      "|SR992-21 Woodlands Ave report v2.0|[]   |\n",
      "+----------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: The vocabulary size should be > 0. Lower minDF as necessary.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114.fit.\n: java.lang.IllegalArgumentException: requirement failed: The vocabulary size should be > 0. Lower minDF as necessary.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7fa374a748e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mtokenized_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mNgramDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0mNgramDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mNgramDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mergeSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_ngram_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: The vocabulary size should be > 0. Lower minDF as necessary.'"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "##############################\n",
    "###### Load The Delta   ######\n",
    "##############################\n",
    "###\n",
    "### Input delta in folder :  /data\n",
    "\n",
    "#\n",
    "job_dir=\"/home/notebookuser/notebooks/cv-ngrams-classifier/job-dir/\"\n",
    "#\n",
    "#datepath=\"2022-02-25\"\n",
    "datepath=datetime.today().strftime('%Y-%m-%d')\n",
    "#\n",
    "my_input_delta_table=job_dir+\"data/delta/json-cv-pdf\"\n",
    "delta_ngram_structure=job_dir+\"data/delta/cv-files-ngrams\"\n",
    "###\n",
    "######\n",
    "##############################Execution##########################\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "# Join with Internal Curation Data in urltopredict staged folder\n",
    "from pyspark.sql import functions as F\n",
    "### use version=1\n",
    "version=1\n",
    "## .option(\"versionAsOf\", version)\n",
    "delta_dataframe_df1=sqlContext.read.format(\"delta\").load(my_input_delta_table)\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#\n",
    "#\n",
    "delta_dataframe_df1.printSchema()\n",
    "delta_dataframe_df1.registerTempTable(\"delta_pdf_cv\")\n",
    "#\n",
    "#### Expose most frequent CV Terms as NGram_n - Group of [1-6] words in CV\n",
    "######\n",
    "####\n",
    "######\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler, RegexTokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def build_ngrams(inputCol=\"pagei\", n=6):\n",
    "\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"pagei\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "\n",
    "    return Pipeline(stages=ngrams + vectorizers + assembler)\n",
    "\n",
    "regexTokenizer = RegexTokenizer(minTokenLength=1, gaps=False, pattern='\\\\w+.w+|\\\\w+/w+|\\\\w+#w+|\\\\w+#|\\\\w+|', inputCol=\"pagei\", outputCol=\"words\", toLowercase=True)\n",
    "\n",
    "#######################################\n",
    "\n",
    "#\n",
    "pages_grouped=sqlContext.sql(\"select * from delta_pdf_cv where 1=1 limit 10 \") #\" where filename IS NOT NULL \")\n",
    "\n",
    "pages_grouped2=pages_grouped.select(col('filename'),explode(\"pages.p_content\").alias(\"pagei\"))\\\n",
    ".select(col('filename'),explode(split(col(\"pagei\"), \"\\s+[1-99]\\' \\'s+\")).alias(\"pagei\")) \n",
    "##[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+[1-99]\\' \\'s+\n",
    "pages_grouped2.printSchema()\n",
    "\n",
    "tokenized_DF = regexTokenizer.transform(pages_grouped2).select(col('Filename'),col(\"words\").alias(\"pagei\")).filter(\"pagei IS NOT NULL \")\n",
    "\n",
    "tokenized_DF.printSchema()\n",
    "tokenized_DF.show(5,0)\n",
    "\n",
    "NgramDF = build_ngrams().fit(tokenized_DF).transform(tokenized_DF)\n",
    "NgramDF.printSchema()\n",
    "NgramDF.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(delta_ngram_structure)\n",
    "#\n",
    "Ngram6DF=NgramDF.groupBy(\"6_grams\").count().orderBy(col('count').desc())      \n",
    "Ngram6DF.show(10,0)\n",
    "#\n",
    "Ngram5DF=NgramDF.groupBy(\"5_grams\").count().orderBy(col('count').desc())      \n",
    "Ngram5DF.show(10,0)\n",
    "#\n",
    "Ngram4DF=NgramDF.groupBy(\"4_grams\").count().orderBy(col('count').desc())      \n",
    "Ngram4DF.show(10,0)\n",
    "#\n",
    "Ngram3DF=NgramDF.groupBy(\"3_grams\").count().orderBy(col('count').desc())      \n",
    "Ngram3DF.show(10,0)\n",
    "#\n",
    "Ngram2DF=NgramDF.groupBy(\"2_grams\").count().orderBy(col('count').desc())      \n",
    "Ngram2DF.show(10,0)\n",
    "#\n",
    "Ngram1DF=NgramDF.groupBy(\"1_grams\").count().orderBy(col('count').desc())      \n",
    "Ngram1DF.show(10,0)\n",
    "#\n",
    "print(\"Load Ngrams to Delta Done\")\n",
    "print(\"Calculate top 10 most frequent 1,2,3,4,5,6 ngrams  - Finished!\")\n",
    "#\n",
    "sc.stop()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- skill: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n",
      "+------+-------------+-----+\n",
      "| skill|         role|level|\n",
      "+------+-------------+-----+\n",
      "|python|data engineer|    2|\n",
      "|python|data engineer|    3|\n",
      "|python|data engineer|    4|\n",
      "|python|data engineer|    5|\n",
      "| scala|data engineer|    3|\n",
      "| scala|data engineer|    4|\n",
      "| scala|data engineer|    5|\n",
      "|  java|data engineer|    2|\n",
      "+------+-------------+-----+\n",
      "only showing top 8 rows\n",
      "\n",
      "+------+-------------+-----+\n",
      "|skill |role         |level|\n",
      "+------+-------------+-----+\n",
      "|python|data engineer|2    |\n",
      "+------+-------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "Skills Load Done!\n",
      "All Loading Jobs Done!\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "###### Import Skills Data : FROM CSV to Delta-Lake ####\n",
    "#######################################################\n",
    "#####\n",
    "####################################################\n",
    "#\n",
    "job_dir=\"/home/notebookuser/notebooks/cv-ngrams-classifier/job-dir/\"\n",
    "#\n",
    "skills_bulk_path=job_dir+\"data/raw_role_skills/*.csv\"\n",
    "#\n",
    "delta_skills_structure=job_dir+\"data/delta/role_skills\"\n",
    "#\n",
    "##############################Execution##########################\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "# Join with Internal Curation Data in urltopredict staged folder\n",
    "from pyspark.sql import functions as F\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"Role_Skills_Dictionary-Ngrams_CVs-Delta\")\n",
    "sqlContext = SQLContext(sc)\n",
    "### remove viewerID\n",
    "internaldata_df1=sqlContext.read.csv(skills_bulk_path,header='true').persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#\n",
    "#\n",
    "internaldata_df1.printSchema()\n",
    "#\n",
    "internaldata_df1.show(8)\n",
    "#\n",
    "df1=internaldata_df1.filter(\"skill IS NOT NULL\").filter(\"role IS NOT NULL\").persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#\n",
    "df1.show(1,0)\n",
    "##\n",
    "df1.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(delta_skills_structure)\n",
    "##\n",
    "print(\"Skills Load Done!\")\n",
    "#\n",
    "#####################################\n",
    "####\n",
    "sc.stop()\n",
    "#\n",
    "print(\"All Loading Jobs Done!\")\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
