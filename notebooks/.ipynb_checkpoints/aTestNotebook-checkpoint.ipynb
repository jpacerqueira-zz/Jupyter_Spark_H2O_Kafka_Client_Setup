{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_221\"; Java(TM) SE Runtime Environment (build 1.8.0_221-b11); Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode)\n",
      "  Starting server from /home/joci/anaconda3/lib/python3.7/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp4q5g3inn\n",
      "  JVM stdout: /tmp/tmp4q5g3inn/h2o_joci_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp4q5g3inn/h2o_joci_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n",
      "Warning: Your H2O cluster version is too old (9 months and 11 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/GMT</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>9 months and 11 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_joci_1fb9d6</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.755 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       Etc/GMT\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.0.1\n",
       "H2O cluster version age:    9 months and 11 days !!!\n",
       "H2O cluster name:           H2O_from_python_joci_1fb9d6\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.755 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  1\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.7.3 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.files.SparkFiles'>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "from pyspark import SparkFiles as spf\n",
    "#\n",
    "import subprocess\n",
    "#### subprocess.run('unset PYSPARK_SUBMIT_ARGS', shell=True)\n",
    "subprocess.run('export SPARK_LOCAL_IP=localhost ', shell=True)\n",
    "#\n",
    "### DELTA Runtime start script \n",
    "##### subprocess.run('export PACKAGES=\"io.delta:delta-core_2.11:0.3.0\" ', shell=True)\n",
    "##### subprocess.run('export PYSPARK_SUBMIT_ARGS=\"--packages ${PACKAGES} ${PYSPARK_SUBMIT_ARGS}\" ', shell=True)\n",
    "####\n",
    "####\n",
    "sc = pyspark.SparkContext(appName=\"A Test 1 App\") \n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "#####\n",
    "######## import subprocess\n",
    "######## subprocess.run('unset http_proxy', shell=True)\n",
    "####\n",
    "h2o.init(ip=\"localhost\",port=54321)\n",
    "#\n",
    "#\n",
    "sparkfiles=spf.getRootDirectory()\n",
    "print(spf)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- viewerid: string (nullable = true)\n",
      " |-- asset: string (nullable = true)\n",
      " |-- device_os: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- asn: string (nullable = true)\n",
      " |-- isp: string (nullable = true)\n",
      " |-- start_time_unix_time: string (nullable = true)\n",
      " |-- startup_time_ms: string (nullable = true)\n",
      " |-- playing_time_ms: string (nullable = true)\n",
      " |-- buffering_time_ms: string (nullable = true)\n",
      " |-- interrupts: string (nullable = true)\n",
      " |-- average_bitrate_kbps: string (nullable = true)\n",
      " |-- startup_error: string (nullable = true)\n",
      " |-- session_tags: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cdn: string (nullable = true)\n",
      " |-- browser: string (nullable = true)\n",
      " |-- conviva_session_id: string (nullable = true)\n",
      " |-- stream_url: string (nullable = true)\n",
      " |-- error_list: string (nullable = true)\n",
      " |-- percentage_complete: string (nullable = true)\n",
      "\n",
      "Data Load Done!\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "###\n",
    "### Input File in folder :  /data \n",
    "my_input_file=\"conviva11.csv\"\n",
    "###\n",
    "######\n",
    "##############################Execution##########################\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "import subprocess\n",
    "### subprocess.run('export SPARK_LOCAL_IP=0.0.0.0', shell=True)\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"IPTV-Anomaly-Detection-Conviva\")\n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "#\n",
    "import subprocess\n",
    "#\n",
    "### DELTA Runtime start script \n",
    "##### subprocess.run('export PACKAGES=\"io.delta:delta-core_2.11:0.3.0\" ', shell=True)\n",
    "##### subprocess.run('export PYSPARK_SUBMIT_ARGS=\"--packages ${PACKAGES} ${PYSPARK_SUBMIT_ARGS}\" ', shell=True)\n",
    "####\n",
    "### subprocess.run('unset http_proxy', shell=True)\n",
    "## TEST IP\n",
    "### my_ip=\"localhost\" ## \"<<MY HOSTNAME DOESN'T WORK>>\" ##\n",
    "### h2o.init(ip=my_ip,port=54321)\n",
    "#\n",
    "#\n",
    "internal_predict_files=\"file:///home/joci/notebooks/data/\"+my_input_file\n",
    "#\n",
    "# Join with Internal Curation Data in urltopredict staged folder\n",
    "from pyspark.sql import functions as F\n",
    "### remove viewerID\n",
    "internaldata_df1=sqlContext.read.csv(internal_predict_files,header='true')\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#\n",
    "#\n",
    "internaldata_df1.printSchema()\n",
    "#\n",
    "df1=internaldata_df1\n",
    "#\n",
    "df1.write.format(\"delta\").save(\"file:///home/joci/notebooks/data/delta_conviva/\")\n",
    "###\n",
    "####\n",
    "sc.stop()\n",
    "#\n",
    "print(\"Data Load Done!\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 10:11:47.289 NotebookApp]\u001b[m JupyterLab extension loaded from /home/joci/anaconda3/lib/python3.7/site-packages/jupyterlab\n",
      "\u001b[32m[I 10:11:47.289 NotebookApp]\u001b[m JupyterLab application directory is /home/joci/anaconda3/share/jupyter/lab\n",
      "\u001b[32m[I 10:11:47.291 NotebookApp]\u001b[m Serving notebooks from local directory: /home/joci/notebooks\n",
      "\u001b[32m[I 10:11:47.291 NotebookApp]\u001b[m The Jupyter Notebook is running at:\n",
      "\u001b[32m[I 10:11:47.291 NotebookApp]\u001b[m http://localhost:8888/?token=fef852edbe012c19703bf8de00d284ff5f1b988fd7b7d251\n",
      "\u001b[32m[I 10:11:47.291 NotebookApp]\u001b[m Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "\u001b[33m[W 10:11:47.318 NotebookApp]\u001b[m No web browser found: could not locate runnable browser.\n",
      "[C 10:11:47.319 NotebookApp] \n",
      "    \n",
      "    To access the notebook, open this file in a browser:\n",
      "        file:///home/joci/.local/share/jupyter/runtime/nbserver-864-open.html\n",
      "    Or copy and paste one of these URLs:\n",
      "        http://localhost:8888/?token=fef852edbe012c19703bf8de00d284ff5f1b988fd7b7d251\n",
      "\u001b[32m[I 10:12:28.448 NotebookApp]\u001b[m 302 GET /?token=fef852edbe012c19703bf8de00d284ff5f1b988fd7b7d251 (127.0.0.1) 3.52ms\n",
      "\u001b[32m[I 10:13:22.442 NotebookApp]\u001b[m 302 GET /?token=fef852edbe012c19703bf8de00d284ff5f1b988fd7b7d251 (127.0.0.1) 2.81ms\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "############################################################# \n",
    "! pyspark --packages io.delta:delta-core_2.11:0.3.0\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
