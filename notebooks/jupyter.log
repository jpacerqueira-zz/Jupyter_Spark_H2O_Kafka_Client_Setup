nohup: appending output to 'nohup.out'
[I 00:01:01.764 NotebookApp] JupyterLab extension loaded from /home/joci/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 00:01:01.765 NotebookApp] JupyterLab application directory is /home/joci/anaconda3/share/jupyter/lab
[I 00:01:01.768 NotebookApp] Serving notebooks from local directory: /home/joci/notebooks
[I 00:01:01.768 NotebookApp] The Jupyter Notebook is running at:
[I 00:01:01.768 NotebookApp] http://(UKPC000117 or 127.0.0.1):9003/?token=da92603f6f7c24d96bd2b153e2654c501e2d7785a99af61f
[I 00:01:01.768 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 00:01:01.794 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///home/joci/.local/share/jupyter/runtime/nbserver-6301-open.html
    Or copy and paste one of these URLs:
        http://(UKPC000117 or 127.0.0.1):9003/?token=da92603f6f7c24d96bd2b153e2654c501e2d7785a99af61f
[I 00:01:57.715 NotebookApp] 302 POST /login?next=%2F (10.0.75.1) 3.21ms
[I 00:01:57.721 NotebookApp] 302 GET / (10.0.75.1) 1.20ms
[I 00:02:04.986 NotebookApp] Kernel started: d82e26a5-ba84-4014-8052-18405e252e00
[I 00:02:08.997 NotebookApp] Adapting to protocol v5.1 for kernel d82e26a5-ba84-4014-8052-18405e252e00
19/07/24 23:02:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 00:02:57.871 NotebookApp] Saving file at /aTestNotebook.ipynb
[I 00:03:10.404 NotebookApp] Saving file at /aTestNotebook.ipynb
[I 00:03:12.741 NotebookApp] Starting buffering for d82e26a5-ba84-4014-8052-18405e252e00:ba5beef5f6c8438e8f7f25a43bb0cdbb
[I 00:03:13.354 NotebookApp] Kernel shutdown: d82e26a5-ba84-4014-8052-18405e252e00
[I 00:03:17.823 NotebookApp] Kernel started: 992df6bf-f92e-4eab-a0e8-5325f17dfc7c
[I 00:03:21.294 NotebookApp] Adapting to protocol v5.1 for kernel 992df6bf-f92e-4eab-a0e8-5325f17dfc7c
19/07/24 23:03:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 00:04:27.268 NotebookApp] Saving file at /aTestNotebook.ipynb
[I 00:04:35.641 NotebookApp] Starting buffering for 992df6bf-f92e-4eab-a0e8-5325f17dfc7c:b732ab1b57c2453286db9eddd64d8879
[I 00:04:41.821 NotebookApp] Kernel shutdown: 992df6bf-f92e-4eab-a0e8-5325f17dfc7c
[I 00:04:45.756 NotebookApp] Kernel started: f64d2e53-15d2-42f5-9caf-12991fbd641e
[I 00:04:49.117 NotebookApp] Adapting to protocol v5.1 for kernel f64d2e53-15d2-42f5-9caf-12991fbd641e
19/07/24 23:05:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 00:05:09.788 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:06:02.626 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:06:24.390 NotebookApp] Starting buffering for f64d2e53-15d2-42f5-9caf-12991fbd641e:d52363730f3343ca98e69db28a34e272
[I 00:06:25.608 NotebookApp] Kernel shutdown: f64d2e53-15d2-42f5-9caf-12991fbd641e
[I 00:11:39.345 NotebookApp] Kernel started: 947d4d53-88ea-4255-83ab-92e83ac9765a
[I 00:11:42.412 NotebookApp] Adapting to protocol v5.1 for kernel 947d4d53-88ea-4255-83ab-92e83ac9765a
[I 00:12:22.765 NotebookApp] Saving file at /aTestNotebook.ipynb
19/07/24 23:12:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
19/07/24 23:12:36 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/07/24 23:12:36 WARN ServletHandler: /jobs/
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/07/24 23:12:36 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/07/24 23:12:36 WARN ServletHandler: /jobs/
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/07/24 23:12:36 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/07/24 23:12:36 WARN ServletHandler: /jobs/
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/07/24 23:12:36 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/07/24 23:12:36 WARN ServletHandler: /jobs/
java.util.NoSuchElementException
	at java.util.Collections$EmptyIterator.next(Collections.java:4191)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:281)
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:38)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:84)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
[I 00:13:06.750 NotebookApp] 302 GET / (127.0.0.1) 3.14ms
[I 00:13:06.774 NotebookApp] 302 GET /tree? (127.0.0.1) 3.36ms
[I 00:13:39.913 NotebookApp] Saving file at /aTestNotebook.ipynb
[I 00:13:58.933 NotebookApp] Saving file at /aTestNotebook.ipynb
[I 00:14:01.018 NotebookApp] Starting buffering for 947d4d53-88ea-4255-83ab-92e83ac9765a:92e15a996e1a4148b824ed98b32f1b95
[I 00:14:02.151 NotebookApp] Kernel shutdown: 947d4d53-88ea-4255-83ab-92e83ac9765a
[I 00:14:12.624 NotebookApp] Kernel started: 6b2f2dc1-5aec-4c26-aea5-bacdef14f520
[I 00:14:16.923 NotebookApp] Adapting to protocol v5.1 for kernel 6b2f2dc1-5aec-4c26-aea5-bacdef14f520
19/07/24 23:15:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 00:15:18.141 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:16:12.402 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:16:52.047 NotebookApp] 302 GET / (10.0.75.1) 11.58ms
[I 00:19:05.635 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:19:10.026 NotebookApp] Starting buffering for 6b2f2dc1-5aec-4c26-aea5-bacdef14f520:8616a164fec94dc597e127dad9647706
[I 00:19:11.152 NotebookApp] Kernel shutdown: 6b2f2dc1-5aec-4c26-aea5-bacdef14f520
[I 00:19:17.719 NotebookApp] Kernel started: 7624cfbd-5a50-4bd7-b568-20663c134631
[I 00:19:21.228 NotebookApp] Adapting to protocol v5.1 for kernel 7624cfbd-5a50-4bd7-b568-20663c134631
19/07/24 23:19:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]19/07/24 23:20:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.
19/07/24 23:20:02 WARN BlockManager: Block rdd_11_0 replicated to only 0 peer(s) instead of 1 peers
                                                                                [I 00:21:17.497 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:22:35.066 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:22:43.200 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:22:44.773 NotebookApp] Starting buffering for 7624cfbd-5a50-4bd7-b568-20663c134631:cfcb9a0770df499c9957aeea54174f70
[I 00:22:46.117 NotebookApp] Kernel shutdown: 7624cfbd-5a50-4bd7-b568-20663c134631
[I 00:23:00.232 NotebookApp] Kernel started: 03006230-bf31-404b-98cd-0c55b6a3fbed
[I 00:23:04.130 NotebookApp] Adapting to protocol v5.1 for kernel 03006230-bf31-404b-98cd-0c55b6a3fbed
[I 00:23:23.184 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:23:27.454 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
19/07/24 23:23:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 1:>                                                          (0 + 1) / 1]19/07/24 23:24:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.
19/07/24 23:24:06 WARN BlockManager: Block rdd_11_0 replicated to only 0 peer(s) instead of 1 peers
                                                                                [I 00:25:00.870 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:26:45.739 NotebookApp] Saving file at /data/conviva-sample.csv
[I 00:27:07.404 NotebookApp] Saving file at /data/conviva11.csv
[I 00:27:19.998 NotebookApp] Saving file at /data/conviva12.csv
[I 00:27:24.434 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:27:35.489 NotebookApp] Starting buffering for 03006230-bf31-404b-98cd-0c55b6a3fbed:77b161eddb154fe68b76f62fe7c4b06a
[I 00:27:36.711 NotebookApp] Kernel shutdown: 03006230-bf31-404b-98cd-0c55b6a3fbed
[I 00:27:45.346 NotebookApp] Kernel started: 6a7b5417-4da6-47d6-a37e-a5c815ade6d7
[I 00:27:48.851 NotebookApp] Adapting to protocol v5.1 for kernel 6a7b5417-4da6-47d6-a37e-a5c815ade6d7
[I 00:27:53.537 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
19/07/24 23:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]19/07/24 23:28:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.
19/07/24 23:28:32 WARN BlockManager: Block rdd_11_0 replicated to only 0 peer(s) instead of 1 peers
                                                                                [I 00:29:45.142 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
[I 00:31:37.730 NotebookApp] 302 GET / (127.0.0.1) 1.02ms
[I 00:31:37.743 NotebookApp] 302 GET /tree? (127.0.0.1) 0.99ms
[I 00:31:45.947 NotebookApp] Saving file at /IPTV_Anomaly_Detection_Conviva_Notebook.ipynb
